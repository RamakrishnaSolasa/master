<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Deploy decisions to DMN Developer Sandbox</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qPxtcQnOdm8/deploy-decisions-to-dmn-developer-sandbox.html" /><author><name>Guilherme Caponetto</name></author><id>https://blog.kie.org/2021/08/deploy-decisions-to-dmn-developer-sandbox.html</id><updated>2021-08-23T18:04:16Z</updated><content type="html">In early 2021, Red Hat introduced the Developer Sandbox for Red Hat OpenShift. On top of that, we built the DMN Developer Sandbox, which makes deploying decision services easy for developers, and this is now live at as part of the Kogito Tooling 0.12.0 release. Let’s check out how the DMN Developer Sandbox works in this post! Photo by on  If you are not familiar with the Developer Sandbox for Red Hat OpenShift, here is a quote from the that pretty much sums up all its capabilities: The sandbox provides you with a private OpenShift environment in a shared, multi-tenant OpenShift cluster that is pre-configured with a set of developer tools. You can easily create containers from your source code or Dockerfile, build new applications using the samples and stacks provided, add services such as databases from our templates catalog, deploy Helm charts, and much more. Discover the rich capabilities of the full developer experience on OpenShift with the sandbox. In addition, I recommend the KIE Live #35, which shows how to stream decisions using both the Developer Sandbox for Red Hat OpenShift and the Red Hat OpenShift Streams for Apache Kafka. Now, let’s talk about the DMN Developer Sandbox. Simply put, the DMN Developer Sandbox allows you to deploy decision models to the Developer Sandbox for Red Hat OpenShift. Here is what happens behind the scenes when you deploy your DMN: * We prepared a base docker image containing an empty Kogito project, a form web app to be loaded up, and tools to build everything up; * All resources associated with the deployment are created for you in your instance, including ImageStream, Service, Route, BuildConfig, Build, and Deployment. * The DMN that is being deployed is placed inside the Kogito project that is in the base image and this project is built as part of a Dockerfile; * Once the project is built up, the generated Quarkus app is started up and exposed through the newly created Route in your instance. Important: The DMN Developer Sandbox is intended to be used during development, so users should not use the deployed DMN services in production or for any type of business-critical workloads. STEP BY STEP: HOW TO DEPLOY YOUR DECISION MODEL 1-) GO TO  You will notice that the toolbar has slightly changed. In the highlighted red rectangle, you will find the KIE Tooling Extended Services status icon, the DMN Developer Sandbox dropdown (as Deploy), the DMN Runner button (as Run), and the Save button. Check out post to learn more about the DMN Runner. 2-) INSTALL THE KIE TOOLING EXTENDED SERVICES Since both DMN Runner and DMN Developer Sandbox features depend on having the KIE Tooling Extended Services running, you need to install it. To do so, click on the Deploy dropdown or on the Run button, and follow the steps that will be presented to you. Once the KIE Tooling Extended Services is running, the status icon will turn green. Note: You need to install the 0.12.0 version of the KIE Tooling Extended Services even if you are using an older version. KIE Tooling Extended Services running. 3-) SETUP YOUR DEVELOPER SANDBOX INFORMATION Click on the Deploy dropdown and then Setup. Since this is the first time we are setting this up, let’s use the guided wizard instead. In the popup, click on the Configure through the guided wizard instead link. Open the guided wizard to configure the Developer Sandbox information. In the first step of the wizard, you need to set up your Developer Sandbox for Red Hat OpenShift instance before proceeding with the rest of the steps. As you will be instructed in the wizard, access the page and create your instance. Developer Sandbox for Red Hat OpenShift fresh instance. Once your instance is ready, you can pick up your username in the upper-right corner and place it in the first step of the wizard. This information is necessary for locating your namespaces (or projects). Note: You receive two namespaces (or projects) when you create your instance, namely username-dev and username-stage. However, we always use the username-dev for the deployments on this first release of the DMN Dev Sandbox. Fill up the first step with the username. In the second step of the wizard, you need to provide some more information about your newly created instance. Following the instructions provided in the wizard, you will reach this page in your instance: Get the host and token information. On this page, you need to copy the –server and –token information, and paste them in the second step of the wizard. This information is necessary for establishing a connection with your instance. Note: Do not share your personal token with anyone. Fill up the second step with host and token. In the next and last step, you should see that your connection has been successfully established using the three pieces of information that you provided (username, host, and token). Connection successfully established in the last step of the wizard. You can then Deploy now or Continue editing. Since our decision model is not ready yet, I will go with Continue editing. Now that everything is set up, you are able to deploy your decision models and check the other deployments that you have done. You will also notice a blue bar at the bottom of the Deploy dropdown button, indicating that your instance is connected. DMN Developer Sandbox connected. 4-) DESIGN YOUR DECISION MODEL In this tutorial, I will be using the Traffic Violation decision model. The cool thing is that, while authoring my decision model, I can use the DMN Runner to be sure that everything is working as expected. Traffic Violation decision model. 5-) DEPLOY YOUR DECISION MODEL Once your decision model is ready, you can go ahead and deploy it. Keep in mind that each deployment takes a few minutes (usually) and it is immutable, i.e., you will need to trigger a new deployment if you make changes in your decision model. Once done, you will find your deployment in your username-dev namespace. Deploy the model and see the deployment in progress. You will see a green indicator icon once your deployment is up and running. Note: The deployment will fail if your decision model contains errors. 6-) CHECK OUT YOUR DEPLOYED DECISION MODEL Once your deployment is up and running, you can access it through the Deploy dropdown. When you do so, a new page will be open on your browser containing the form associated with your model for you to test and share the URL with others. Open the form associated with the deployed decision model. Test the deployed decision model. You can also access the Swagger UI associated with the deployed decision model and share the URL with others. Access the Swagger UI associated with the deployed decision model. Lastly, you can even open your decision model on and share the URL with others. SOME THINGS TO KEEP IN MIND * Your Developer Sandbox for Red Hat OpenShift instance needs to be renewed every 30 days. It means that, after 30 days, all your data will be deleted. * Tokens need to be renewed daily. * Limit of 10 deployments. * Deployments stay up for 8 hours but you can scale them up again when they go down. I would like to recommend the KIE Live #40 if you want to see more details and a full end-to-end demonstration on how to use the DMN Developer Sandbox. And that’s all for today. Thanks for reading! &#x1f603; The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qPxtcQnOdm8" height="1" width="1" alt=""/&gt;</content><dc:creator>Guilherme Caponetto</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/deploy-decisions-to-dmn-developer-sandbox.html</feedburner:origLink></entry><entry><title type="html">Kogito Tooling 0.12.0 Released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/n_QbD0xh5_U/kogito-tooling-0-12-0-released.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/08/kogito-tooling-0-12-0-released.html</id><updated>2021-08-23T05:00:00Z</updated><content type="html">We have just launched a fresh new Kogito Tooling release! &#x1f389; On the 0.12.0 , we made a lot of improvements and bug fixes. We are also happy to announce that this release marks the first iteration of our ‘DMN deploy on OpenShift’ feature, and also we have a lot of improvements on our DMN and BPMN Editors. This post will give a quick overview of this . I hope you enjoy it! DEPLOY DECISIONS TO DMN DEVELOPER SANDBOX ON DMN.NEW We just launched in our dmn.new environment a feature that allows you to quickly deploy decision models to the Developer Sandbox for Red Hat OpenShift. This feature is super cool, and I invite you to give it a try. You can also check more details on this blog . DMN NODES ARE NOT CREATED ON TOP OF THE SELECTED NODE DMN diagrams are generally vertical (whereas BPMN is horizontal). When adding a “Decision Node” from a “DMN Input Data,” for better usability, now the nodes are created on top of the selected node. DMN SUPPORT FOR BEND-POINTS ON CONNECTORS We also added support for bend-points on the DMN diagram that are pretty useful, especially on Complex DMN diagrams. See this for more details. LINE SPLICING FOR BPMN AND DMN EDITORS Line splicing is a new feature that allows dropping an existing node on top of a connector and automatically split it into two new connectors. This was included in both our DMN and BPMN editors. Soon we will publish a blog post with a detailed description of this feature. RESIZE CONTROL POINTS IMPROVEMENTS We made many improvements on resizing control points for our BPMN and DMN editor, including changing the resize icon and modifying how magnets react on a resize. SUPPORT FOR PROCESS METADATA ATTRIBUTES We also added a new AdvancedData that allows users to add generic metadata to all node types and event types in the BPMN editor. [![Metadata](/assets/2021/metatada.png “Metadata * – BPMN Editor – Support for node/events metadata attributes * – DMN Developer Sandbox for Red Hat OpenShift * – DMN target position is not stored * – Stunner – Task Resize option doesn’t show up * – [DMN Editor] Ctrl-B always converts field to structure and nests * – VSCode DMN, BPMN editor – creating connection can’t be cancelled easily * – Stunner – Resize Icon remains displayed * – BPMN Editor – Cannot import some processes * – DMN Runner – Wizard step for running * – BPMN Editor – Marshallers encoding issues * – [Test Scenario] No effects when assigning a not-expression Simple Type column to expression type (and viceversa) * – BPMN Editor – Moving connector’s bendpoints results on erros in the console * – [Stunner] bend point modification causes diagram inaccessible * – Stunner – Line splicing * – Implement E2E automation for Reuse of Data Types in BPMN Designer * – Verify support for node/event metadata attribues feature * – Stunner – first POC of new marshallers * – [DMN Designer] When users create a node by using a shortcut, it’s not created above * – Update vscode-extension-tester to 4.1.0 * – [DMN/BPMN] Wired web apps – Fix doc screenshot * – Implement – designs for orthogonal lines between diagram nodes * – [Test Scenario] – Errors when executing models using imported inputs and/or decisions nodes * – Stunner – Make new nodes editable automatically * – Stunner – Resize control points – Fixes &amp;amp; UX improvements * – [DMN Designer] Add support for bend-points on connectors * – [Stunner] Lienzo – Migration to native interfaces * – Stunner – Alignment helpers missing during node resize * – Stunner – WID files with comments and Imports can’t be loaded FURTHER READING/WATCHING We had some excellent blog posts on Kie Blog that I recommend you read: * , by Eder Ignatowicz; * , by Manaswini Das; * , by Daniel José dos Santos; * , by Luiz Motta; * , by Manaswini Das; * , by Valentino Pellegrino; * , by Guilherme Caponetto. We also presented in some Kie Lives: * , by Tiago Bento; * , by Yeser Amer; THANK YOU TO EVERYONE INVOLVED! I would like to thank everyone involved with this release, from the excellent KIE Tooling Engineers to the lifesavers QEs and the UX people that help us look awesome! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/n_QbD0xh5_U" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/kogito-tooling-0-12-0-released.html</feedburner:origLink></entry><entry><title>Composable software catalogs on Kubernetes: An easier way to update containerized applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Qu9uHZyp9gE/composable-software-catalogs-kubernetes-easier-way-update-containerized" /><author><name>David Festal</name></author><id>d42b5c6a-cc3d-45ed-9bd9-ac7ffb22b852</id><updated>2021-08-20T07:00:00Z</updated><published>2021-08-20T07:00:00Z</published><summary type="html">&lt;p&gt;Recently, I've been experimenting with how to build and use composable software catalogs on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Similar to &lt;a href="https://developers.redhat.com/products/softwarecollections/overview"&gt;Red Hat Software Collections&lt;/a&gt; for &lt;a href="products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;, but adapted for a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; context, composable software catalogs let developers add tooling without building a new container image.&lt;/p&gt; &lt;p&gt;This article explains how composable software catalogs use existing container technologies to build on the Software Collections model, and how they can potentially make more options available to container users, simplify builds, and reduce container image sizes.&lt;/p&gt; &lt;h2&gt;Software Collections in a containerized world&lt;/h2&gt; &lt;p&gt;To understand the need for composable software catalogs, let's go back in time a bit.&lt;/p&gt; &lt;p&gt;Do you remember &lt;a href="https://www.softwarecollections.org/en/"&gt;Software Collections&lt;/a&gt;? The motto of this project, backed by Red Hat, was: &lt;em&gt;All versions of any software on your system. Together.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The promise was to build, install, and use multiple versions of software on the same system, without affecting system-wide installed packages. The key point was to create this multifold environment without affecting system-wide installed packages. In other words, it provided additional tooling without any change to the current state of the operating system as a whole. Software Collections worked well in its time, even winning a Top Innovator Award at DeveloperWeek 2014.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Red Hat Software Collections is available for Red Hat Enterprise Linux 7 and earlier supported releases. Starting with Red Hat Enterprise Linux 8, &lt;a href="https://access.redhat.com/support/policy/updates/rhscl"&gt;application streams replace Software Collections&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;A new landscape but the same need&lt;/h3&gt; &lt;p&gt;Things have changed since 2014. The container revolution popped up and brought features such as execution isolation, file system layering, and volume mounting. This solved quite a lot of problems. Thanks to containers, one could say that the old Software Collections became obsolete. But container orchestrators came along, as well (I'll stick to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; in this article). Deploying workloads as containers inside pods became standard. Finally, even workloads such as build pipelines or IDEs moved to the cloud and also ran inside containers.&lt;/p&gt; &lt;p&gt;But containers themselves have limitations. At some point, developers start experiencing the same type of need inside containers that Software Collections once tried to solve at the operating system level.&lt;/p&gt; &lt;h3&gt;Why revisit Software Collections?&lt;/h3&gt; &lt;p&gt;A &lt;em&gt;container&lt;/em&gt; is based on a single &lt;em&gt;container image&lt;/em&gt;, which is like a template for multiple identical containers. And a container image is optionally based on a single &lt;em&gt;container image&lt;/em&gt; &lt;em&gt;parent&lt;/em&gt;. To build a container image, you typically start from a basic operating system image. Then you add layers one by one, each on top of the previous one, to provide each additional tool or feature that you need in your container. Thus, each container is based on an image whose layers are overlays in a single inheritance tree. A container image is a snapshot of the current state of an operating system at a given point in time.&lt;/p&gt; &lt;p&gt;For container images, the old promise of Software Collections would be useful. In a container context, the goal of providing additional tooling &lt;em&gt;without any change to the current state of the operating system&lt;/em&gt; simply becomes &lt;em&gt;without having to build a new container image&lt;/em&gt;.&lt;/p&gt; &lt;h3&gt;A combinatorial explosion of components&lt;/h3&gt; &lt;p&gt;Let's take an example:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;I'd like to run a &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; application—let's say the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/tree/master/getting-started"&gt;getting started example&lt;/a&gt;—directly from source code, in development mode. I will need at least a JDK version and a Maven version on top of the base operating system.&lt;/li&gt; &lt;li&gt;I'd also like to test the application with the widest available range of versions and flavors of the JDK, Maven, and the base operating system.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For each combination of the possible variants for those three components (JDK, Maven, and operating system), I would need to build a dedicated container image. And what if I also wanted to test with as many Gradle versions as possible? Not to mention including the native build use case, which requires GraalVM. Now imagine the combinatorial explosion that will occur if I decide to also include arbitrary versions of all my preferred tools.&lt;/p&gt; &lt;h3&gt;Inheritance versus composition&lt;/h3&gt; &lt;p&gt;The current manner of building containers limits us to a &lt;em&gt;single-inheritance model &lt;/em&gt;when what we need is &lt;em&gt;composition&lt;/em&gt;. Sometimes it would be great to be able to compose additional features or tools inside a container, without having to build a new container image. In fact, we just need to &lt;em&gt;compose container images&lt;/em&gt; at runtime. Obviously, allowing that in full generality seems tricky (if not impossible) to implement, at least given the current state of Kubernetes and containers. But what about a more limited case where we would only inject external self-contained tooling or read-only data into an existing container?&lt;/p&gt; &lt;h2&gt;Toward composable software catalogs on Kubernetes&lt;/h2&gt; &lt;p&gt;Injecting external self-contained tooling or read-only data into a container at runtime would obviously be particularly relevant if you think of things such as &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;, Maven, Gradle, even &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, NPM, Typescript, and the growing number of self-contained &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; utilities like Kubectl and Helm, as well as the Knative or Tekton CLI tools. None of them requires an "installation" process, strictly speaking. In order to start using them on most &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; variants of a given platform, you need only to download and extract them.&lt;/p&gt; &lt;h3&gt;Combining two container technologies&lt;/h3&gt; &lt;p&gt;Now let's introduce two container technologies that will allow us to implement this tool injection at runtime:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/"&gt;Container Storage Interface (CSI)&lt;/a&gt;, and more specifically &lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/"&gt;CSI Inline Ephemeral Volumes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://buildah.io/"&gt;Buildah&lt;/a&gt; containers&lt;/li&gt; &lt;/ul&gt;&lt;h4&gt;Container Storage Interface&lt;/h4&gt; &lt;p&gt;According to the &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/#why-csi"&gt;Kubernetes documentation&lt;/a&gt;:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;em&gt;CSI was developed as a standard for exposing arbitrary block and file storage storage systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes. With the adoption of the Container Storage Interface, the Kubernetes volume layer becomes truly extensible. Using CSI, third-party storage providers can write and deploy plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code. This gives Kubernetes users more options for storage and makes the system more secure and reliable.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;CSI opens many doors to implementing and integrating storage solutions into Kubernetes. On top of that, the &lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/"&gt;CSI Ephemeral Inline Volumes&lt;/a&gt; feature, still in beta, for now, allows you to specify a CSI volume, along with its parameters, directly in the pod spec, and only there. This is perfect to allow references, directly inside the pod, to the name of a tool to inject into pod containers.&lt;/p&gt; &lt;h4&gt;Buildah containers&lt;/h4&gt; &lt;p&gt;The &lt;code&gt;buildah&lt;/code&gt; tool is a well-known CLI tool that facilitates building Open Container Initiative (OCI) container images. Among many other features, it provides two that are very interesting for us:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Creating a container (from an image) that is not executing any command at the start, but can be manipulated, completed, and modified to possibly create a new image from it.&lt;/li&gt; &lt;li&gt;Mounting such a container to gain access to its underlying file system.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Buildah containers as CSI volumes&lt;/h2&gt; &lt;p&gt;The first attempt at combining CSI and &lt;code&gt;buildah&lt;/code&gt; started as a prototype example by the Kubernetes-CSI contributors, &lt;a href="https://github.com/kubernetes-csi/csi-driver-image-populator"&gt;csi-driver-image-populator&lt;/a&gt;. It was my main inspiration for the work shown in this article.&lt;/p&gt; &lt;p&gt;Providing a very lightweight and simple CSI driver, with the &lt;code&gt;image.csi.k8s.io&lt;/code&gt; identifier, &lt;code&gt;csi-driver-image-populator&lt;/code&gt; allows container images to be mounted as volumes. Deployed with a DaemonSet, the driver runs on each worker node of the Kubernetes cluster and waits for volume-mount requests. In the following example, a container image reference is specified in the pod as a parameter of the &lt;code&gt;image.csi.k8s.io&lt;/code&gt; CSI volume. Using &lt;code&gt;buildah&lt;/code&gt;, the corresponding CSI driver pulls the image, creates a container from it, and mounts its file system. The &lt;code&gt;buildah&lt;/code&gt; container filesystem is thus available to mount directly as a pod volume. Finally, the pod containers can reference this pod volume and use it:&lt;/p&gt; &lt;pre&gt; apiVersion: v1 kind: Pod metadata: name: example spec: containers: - name: main image: main-container-image volumeMount: - name: &lt;strong&gt;composed-container-volume&lt;/strong&gt; mountPath: /somewhere-to-add-the-composed-container-filesystem volumes: - name: &lt;strong&gt;composed-container-volume&lt;/strong&gt; csi: driver: image.csi.k8s.io volumeAttributes: image: &lt;strong&gt;composed-container-image&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;Upon pod removal, the pod volume is unmounted by the driver, and the &lt;code&gt;buildah&lt;/code&gt; container is removed.&lt;/p&gt; &lt;h3&gt;Adapting the CSI driver for composable software catalogs&lt;/h3&gt; &lt;p&gt;Some aspects of the &lt;code&gt;csi-driver-image-populator&lt;/code&gt; prototype do not fit our use case for composable software catalogs:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;We don't need containers in the pod to have write access to composed image volumes. The whole idea in this article is to inject &lt;em&gt;read-only&lt;/em&gt; tools and data to the pod containers through the CSI inline volumes.&lt;/li&gt; &lt;li&gt;Sticking to the read-only use case allows us to use a single &lt;code&gt;buildah&lt;/code&gt; container for a given tool image, and share its mounted file system with all the pods that reference it. The number of &lt;code&gt;buildah&lt;/code&gt; containers then depends only on the number of images provided by the software catalog on the CSI driver side. This opens the door to additional performance optimizations.&lt;/li&gt; &lt;li&gt;For both performance and security reasons, we should avoid automatically pulling the container image mounted as a CSI inline volume. Let's pull images by an external component, outside the CSI driver. And let the CSI driver expose only images that were already pulled. Thus we limit the mounted images to a well-defined list of known images. In other words, we stick to a &lt;em&gt;managed software catalog&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Finally, for Kubernetes clusters that use an OCI-conformant container runtime (&lt;a href="https://cri-o.io/"&gt;cri-o&lt;/a&gt;, for example), we should be able to reuse images already pulled by the Kubernetes container runtime on the cluster node. This would take advantage of the image pulling capability of the Kubernetes distribution and comply with its configuration, instead of using a dedicated, distinct mechanism and configuration to pull a new image.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To validate the idea described in this article, the changes just listed were implemented in a newly created CSI driver named &lt;a href="https://github.com/katalogos/csi-based-tool-provider"&gt;csi-based-tool-provider&lt;/a&gt;, starting from the &lt;code&gt;csi-driver-image-populator&lt;/code&gt; prototype to bootstrap the code.&lt;/p&gt; &lt;h3&gt;Providing dedicated tooling images&lt;/h3&gt; &lt;p&gt;In general, the new &lt;code&gt;csi-based-tool-provider&lt;/code&gt; driver is able to mount, as a pod read-only volume, any file system subpath of any container image. But still, it would be useful to define a typical structure for the container images that would populate such a software catalog. For "no-installation" software such as Java, which is simply delivered as an archive to extract, the most straightforward way to populate the catalog is to use "from scratch" images with the software directly extracted at the root of the filesystem. An example of a &lt;code&gt;Dockerfile&lt;/code&gt; for the &lt;a href="https://developers.redhat.com/products/openjdk/overview"&gt;OpenJDK&lt;/a&gt; 11 image would be:&lt;/p&gt; &lt;pre&gt; FROM registry.access.redhat.com/ubi8/ubi as builder WORKDIR /build RUN curl -L https://github.com/AdoptOpenJDK/openjdk11-binaries/releases/download/jdk-11.0.9.1%2B1/OpenJDK11U-jdk_x64_linux_hotspot_11.0.9.1_1.tar.gz | tar xz FROM scratch WORKDIR / COPY --from=builder /build/jdk-11.0.9.1+1 . &lt;/pre&gt; &lt;p&gt;The same holds true for the Maven distribution required by our Quarkus example mentioned earlier. Next, we'll use the Quarkus example as a proof of concept (POC).&lt;/p&gt; &lt;h2&gt;Using composable software catalogs with Quarkus&lt;/h2&gt; &lt;p&gt;Now let's come back to our Quarkus example. I want to use only an interchangeable basic operating system for my container, without building any dedicated container image. And now I can manage additional tooling through CSI volume mounts on images from my new composable software catalog.&lt;/p&gt; &lt;p&gt;The full deployment looks like this:&lt;/p&gt; &lt;pre&gt; apiVersion: apps/v1 kind: Deployment metadata: name: csi-based-tool-provider-test spec: selector: matchLabels: app: csi-based-tool-provider-test replicas: 1 template: metadata: labels: app: csi-based-tool-provider-test spec: initContainers: - name: git-sync image: k8s.gcr.io/git-sync:v3.1.3 volumeMounts: - name: source mountPath: /tmp/git env: - name: HOME value: /tmp - name: GIT_SYNC_REPO value: https://github.com/quarkusio/quarkus-quickstarts.git - name: GIT_SYNC_DEST value: quarkus-quickstarts - name: GIT_SYNC_ONE_TIME value: "true" - name: GIT_SYNC_BRANCH value: 'main' containers: - name: main image: registry.access.redhat.com/ubi8/ubi args: - ./mvnw - compile - quarkus:dev - -Dquarkus.http.host=0.0.0.0 workingDir: /src/quarkus-quickstarts/getting-started ports: - containerPort: 8080 env: - name: HOME value: /tmp - name: JAVA_HOME value: /usr/lib/jvm/jdk-11 - name: M2_HOME value: /opt/apache-maven-3.6.3 volumeMounts: - name: java mountPath: /usr/lib/jvm/jdk-11 - name: maven mountPath: /opt/apache-maven-3.6.3 - name: source mountPath: /src volumes: - name: java csi: driver: toolprovider.csi.katalogos.dev volumeAttributes: image: quay.io/dfestal/csi-tool-openjdk11u-jdk_x64_linux_hotspot_11.0.9.1_1:latest - name: maven csi: driver: toolprovider.csi.katalogos.dev volumeAttributes: image: quay.io/dfestal/csi-tool-maven-3.6.3:latest - name: source emptyDir: {} &lt;/pre&gt; &lt;p&gt;To clone the example source code from GitHub, I reuse the &lt;a href="https://github.com/kubernetes/git-sync"&gt;git-sync&lt;/a&gt; utility inside an &lt;code&gt;initContainer&lt;/code&gt; of my Kubernetes &lt;code&gt;Deployment&lt;/code&gt;, but that's just for the sake of laziness and doesn't relate to the current work.&lt;/p&gt; &lt;h3&gt;Making tools available&lt;/h3&gt; &lt;p&gt;The first real interesting part of the implementation is:&lt;/p&gt; &lt;pre&gt; ... volumes: - name: java csi: driver: toolprovider.csi.katalogos.dev volumeAttributes: image: quay.io/dfestal/csi-tool-openjdk11u-jdk_x64_linux_hotspot_11.0.9.1_1:latest - name: maven csi: driver: toolprovider.csi.katalogos.dev volumeAttributes: image: quay.io/dfestal/csi-tool-maven-3.6.3:latest ... &lt;/pre&gt; &lt;p&gt;This configuration uses the new CSI driver to expose my two &lt;a href="https://github.com/katalogos/csi-based-tool-provider/tree/master/examples/catalog"&gt;tooling images&lt;/a&gt; as CSI read-only volumes.&lt;/p&gt; &lt;h3&gt;Mounting the tools&lt;/h3&gt; &lt;p&gt;The following configuration makes Java and Maven installations available for the main pod container to mount them at the needed place:&lt;/p&gt; &lt;pre&gt; ... containers: - name: main ... env: ... - name: JAVA_HOME value: /usr/lib/jvm/jdk-11 - name: M2_HOME value: /opt/apache-maven-3.6.3 volumeMounts: - name: java mountPath: /usr/lib/jvm/jdk-11 - name: maven mountPath: /opt/apache-maven-3.6.3 ... &lt;/pre&gt; &lt;p&gt;Note that the pod container owns the final path where the Java and Maven installations will be mounted. So the pod container can also set the related environment variables to the right paths.&lt;/p&gt; &lt;h3&gt;Using the mounted tools&lt;/h3&gt; &lt;p&gt;Finally, the container that will build and run the application source code in development mode can be based on a bare operating system image, and has nothing more to do than call the &lt;a href="https://quarkus.io/guides/getting-started#running-the-application"&gt;recommended startup command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; ... - name: main image: registry.access.redhat.com/ubi8/ubi args: - ./mvnw - compile - quarkus:dev - -Dquarkus.http.host=0.0.0.0 workingDir: /src/quarkus-quickstarts/getting-started ... &lt;/pre&gt; &lt;p&gt;The example will start on a &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Image&lt;/a&gt;. But the great thing is that you can make changes, such as switching to an Ubuntu image, and the server will start and run the same way without any other change. And if you want to switch to another version of Maven, just change the reference to the corresponding container image in the &lt;code&gt;maven&lt;/code&gt; CSI volume.&lt;/p&gt; &lt;p&gt;If you scale up this deployment to ten pods, the same underlying Java and Maven installations will be used. No files will be duplicated on the disk, and no additional containers will be created on the cluster node. Only additional bind mounts will be issued on the cluster node. And the space savings will be the same, however many workloads use the Java and Maven tooling images on this node.&lt;/p&gt; &lt;h3&gt;What about performance?&lt;/h3&gt; &lt;p&gt;In the very first implementation, the new &lt;code&gt;csi-based-tool-provider&lt;/code&gt; driver ran &lt;a href="https://github.com/containers/buildah/blob/master/docs/buildah-manifest.md"&gt;buildah manifest&lt;/a&gt; commands to store the various metadata related to mounted images, along with the associated containers and volumes, inside an &lt;a href="https://github.com/opencontainers/image-spec/blob/master/manifest.md#oci-image-manifest-specification"&gt;OCI manifest&lt;/a&gt;. Although this design was useful to get a POC working quickly, it required hard locks on the whole CSI mounting and unmounting process (&lt;code&gt;NodePublishVolume&lt;/code&gt; and &lt;code&gt;NodeUnpublishVolume&lt;/code&gt; CSI requests), in order to avoid concurrent modification of this global index and ensure consistency. Moreover, the &lt;code&gt;buildah&lt;/code&gt; container was initially created on the fly at mount time if necessary, and as soon as a given tool was not mounted by any pod container anymore, the corresponding &lt;code&gt;buildah&lt;/code&gt; container was removed by the CSI driver.&lt;/p&gt; &lt;p&gt;This design could lead to a mount delay of several seconds, especially when mounting an image for the first time. Instead of that design, the driver now uses an embeddable, high-performance, transactional key-value database called &lt;a href="https://github.com/dgraph-io/badger"&gt;BadgerDB&lt;/a&gt;. This choice allows much better performance and less contention caused by read-write locks. In addition, the list of container images exposed to the driver is now configured through a &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#mounted-configmaps-are-updated-automatically"&gt;mounted ConfigMap&lt;/a&gt;. Images, as well as their related &lt;code&gt;buildah&lt;/code&gt; containers, are managed, created, and cleaned up in background tasks. These two simple changes have reduced the average volume mount delay to some fractions of a second, as shown by the graph of the related &lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; metric in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/lJzq7mN.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/lJzq7mN.png?itok=k7FMYerB" width="556" height="265" alt="Composable software catalogs on Kubernetes: Average volume mount delay for a tool is between 15 and 20 ms." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The average volume mount delay for updated containers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Average volume mount delay for updated containers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;On a local &lt;a href="https://minikube.sigs.k8s.io/docs/"&gt;Minikube&lt;/a&gt; installation, for a simple pod containing one mounted CSI volume with the JDK image mentioned earlier, and one very simple container (doing nothing more than listing the content of the mounted volume and then sleeping), the average delay required to mount the JDK inside the Pod fluctuated between 15 and 20 milliseconds. In comparison with the overall pod startup duration (between 1 and 3 seconds), this is pretty insignificant.&lt;/p&gt; &lt;h3&gt;Testing the example&lt;/h3&gt; &lt;p&gt;The related code is available in the &lt;a href="https://github.com/katalogos/csi-based-tool-provider"&gt;csi-based-tool-provider&lt;/a&gt; GitHub repository, including instructions on how to test it using pre-built container images.&lt;/p&gt; &lt;h2&gt;Additional use cases for composable software catalogs&lt;/h2&gt; &lt;p&gt;Beyond the example used in this article, we can foresee concrete use cases where such tool injection would be useful. First, it reduces the combinatorial-explosion effect of having to manage, in a single container image, the versioning and lifecycle of both the underlying system and all the various system-independent tools. So it could reduce the overall size of image layers stored on Kubernetes cluster nodes.&lt;/p&gt; &lt;h3&gt;Red Hat OpenShift Web Terminal&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://www.openshift.com/blog/a-deeper-look-at-the-web-terminal-operator-1"&gt;Red Hat OpenShift Web Terminal&lt;/a&gt; is an example of a tool that could benefit from a software catalog. When opening a web terminal, the OpenShift console starts a pod with a container embedding all the typically required CLI tools. But if you need additional tools, you will have to replace this default container image with your own customized one, built by your own means. This build would not be necessary if we could provide all the CLI tools as volumes in a basic container. Composable software catalogs would also relieve the &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration&lt;/a&gt; (CI) burden of having to rebuild the all-in-one container image each time one of the tools has to be updated. Going one step further, a catalog should allow using, in the web terminal, exactly the same version of the Kubernetes-related command-line tools (like &lt;code&gt;oc&lt;/code&gt; and &lt;code&gt;kubectl&lt;/code&gt;) as the version of the underlying OpenShift cluster.&lt;/p&gt; &lt;h3&gt;Tekton pipelines&lt;/h3&gt; &lt;p&gt;I also imagine how composable software catalogs could be used to inject off-the-shelf build tools into &lt;a href="https://github.com/tektoncd/pipeline/blob/master/docs/tasks.md#defining-steps"&gt;Tekton Task Steps&lt;/a&gt;. Here as well, there would be no more need to change and possibly rebuild Step container images each time you want to run your pipeline with different build tool variants or versions.&lt;/p&gt; &lt;h3&gt;Cloud IDEs&lt;/h3&gt; &lt;p&gt;Last but not least, composable software catalogs could benefit the various cloud-enabled IDEs, such as &lt;a href="https://www.eclipse.org/che/"&gt;Eclipse Che.&lt;/a&gt; The catalogs would make it really easy to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Efficiently switch the Java or Maven installations in a workspace&lt;/li&gt; &lt;li&gt;Share these installations among the various containers&lt;/li&gt; &lt;li&gt;Have several versions at the same time&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here as well, this new approach could greatly reduce the CI burden. We could stop building and maintaining a container image for each combination of underlying OS and tools. And composable software catalogs would finally unlock the combination of developer tools at runtime according to the developer's needs.&lt;/p&gt; &lt;h2&gt;What next?&lt;/h2&gt; &lt;p&gt;Although the proof of concept presented in this article is in an early alpha stage, we can already imagine some of the next steps to move it forward.&lt;/p&gt; &lt;h3&gt;Welcoming Katalogos&lt;/h3&gt; &lt;p&gt;A lot can be built on the foundation of the &lt;code&gt;csi-based-tool-provider&lt;/code&gt;. But as a first step, we should certainly set up a wider project dedicated to Kubernetes composable software catalogs. The CSI driver would be its first core component. So we've called this project Katalogos, from the ancient Greek word for a catalog: a register, especially one used for enrollment.&lt;/p&gt; &lt;h3&gt;Packaging the project as a complete solution&lt;/h3&gt; &lt;p&gt;Once the wider Katalogos project is bootstrapped, these next steps come to mind:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Add a Software Catalog Manager component to organize, pull, and manage images as software catalogs and make them available to the CSI driver on each cluster node.&lt;/li&gt; &lt;li&gt;Build an operator to install the CSI driver as well as configure the Software Catalog Manager.&lt;/li&gt; &lt;li&gt;Define a way to easily inject the required CSI volumes, as well as related environment variables, into pods according to annotations.&lt;/li&gt; &lt;li&gt;Provide related tooling and processes to easily build software catalogs that can feed the Software Catalog Manager.&lt;/li&gt; &lt;li&gt;Extend the mechanism to support the more complex case of software packages that are not inherently self-contained.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Getting feedback and building a community&lt;/h3&gt; &lt;p&gt;This article presented some ideas, with a minimal proof of concept, for a project that, I believe, could meet a real need in the current state of cloud-native development. The article is also a bid to get feedback, spark interest, and gather other use cases where the concept would fit. So, please comment, try the examples, open issues, fork the GitHub repository ... or simply star it.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/20/composable-software-catalogs-kubernetes-easier-way-update-containerized" title="Composable software catalogs on Kubernetes: An easier way to update containerized applications"&gt;Composable software catalogs on Kubernetes: An easier way to update containerized applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Qu9uHZyp9gE" height="1" width="1" alt=""/&gt;</summary><dc:creator>David Festal</dc:creator><dc:date>2021-08-20T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/20/composable-software-catalogs-kubernetes-easier-way-update-containerized</feedburner:origLink></entry><entry><title type="html">Season 3 and 60th Insights episode</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ansEqTkd-_k/" /><author><name /></author><id>https://quarkus.io/blog/60th-quarkus-insights/</id><updated>2021-08-20T00:00:00Z</updated><content type="html">After a summer break and little bit of COVID-19 delay, we will finally have the 60th(!) Quarkus Insights episode on Monday the 23rd August. For those who don’t know, Quarkus Insights is a (almost) weekly video/audio podcast where we host people from all parts of the Quarkiverse to sit down...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ansEqTkd-_k" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/60th-quarkus-insights/</feedburner:origLink></entry><entry><title>Cluster tooling updates and more in Red Hat OpenShift's Web Terminal Operator 1.3</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/J3giX7F6Bw4/cluster-tooling-updates-and-more-red-hat-openshifts-web-terminal-operator-13" /><author><name>Josh Pinkney, Angel Misevski</name></author><id>fb68ba28-3337-4201-86cb-5d45789412d4</id><updated>2021-08-19T07:00:00Z</updated><published>2021-08-19T07:00:00Z</published><summary type="html">&lt;p&gt;The Web Terminal Operator in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; provides a web terminal with common cluster tooling pre-installed. The operator gives you the power and flexibility to work with your product directly through the OpenShift web console, eliminating the need to have all your tooling installed locally.&lt;/p&gt; &lt;p&gt;This article is an overview of the new features introduced in Web Terminal Operator 1.3. These improvements include depending on the newly released DevWorkspace Operator, adding support for saving your home directory, and updating our tooling to be compatible with OpenShift 4.8.&lt;/p&gt; &lt;h2&gt;DevWorkspace Operator dependency&lt;/h2&gt; &lt;p&gt;Previously, the Web Terminal Operator relied on an embedded version of the &lt;a href="https://github.com/devfile/devworkspace-operator/"&gt;DevWorkspace controller&lt;/a&gt; to provide support for web terminals. That meant that in order to get the latest and greatest changes for the DevWorkspace controller, you had to wait three months for a new Web Terminal Operator release. As of Web Terminal 1.3, we have extracted that dependency, released the DevWorkspace Operator as its own separate operator, and now depend on that for providing support for web terminals. What this means for you is that the engine “under the hood” (the DevWorkspace Operator) will be updated every six weeks, instead of the standard three-month schedule for the Web Terminal Operator.&lt;/p&gt; &lt;h2&gt;Saving your home directory&lt;/h2&gt; &lt;p&gt;With Web Terminal 1.3, you will be able to mount your home directory and persist changes to your web terminal over multiple restarts. This feature isn’t enabled by default and it requires some additional configuration. This additional configuration depends on whether or not you already have a web terminal.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Only a web terminal that has a volume and volume mount for &lt;code&gt;/home/user&lt;/code&gt; will persist the home directory. Details follow.&lt;/p&gt; &lt;h3&gt;If you already have a web terminal&lt;/h3&gt; &lt;p&gt;In order to persist the home directory over multiple restarts, you will need to update the custom resource to mount a volume into the web terminal tooling container:&lt;/p&gt; &lt;p&gt;First, get your web terminal custom resource in your namespace and open it up for editing:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get dw -n ${your_namespace} oc edit dw ${your web terminal name} -n ${your_namespace}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that you have your custom resource open for editing, modify the DevWorkspace custom resource components section to add a mount for the storage. This can be done by first defining a volume in &lt;code&gt;spec.template.components&lt;/code&gt; and then mounting that volume in the web terminal tooling container. The highlighted portion of the YAML file in Figure 1 shows what you need to add in order to get the mount.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/terminal_mount.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/terminal_mount.png?itok=n82YvfnY" width="580" height="612" alt="The configuration of your DevWorkspace must have a section mounting the /home/user directory" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Configuration file with a section mounting the /home/user directory. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Once your custom resource has been edited and saved, you can start a new web terminal and create changes under &lt;code&gt;/home/user&lt;/code&gt;. Your changes will persist over multiple restarts.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are editing the custom resource inside a web terminal, the web terminal will automatically apply the changes and then close the active web terminal. To reopen the web terminal with the changes applied, click the &lt;strong&gt;Restart Terminal&lt;/strong&gt; button.&lt;/p&gt; &lt;h3&gt;If you don’t already have a web terminal&lt;/h3&gt; &lt;p&gt;In order to persist the home directory if you don’t already have a web terminal, you can edit your configuration file and add the following fields. In the YAML shown, replace &lt;code&gt;${your namespace}&lt;/code&gt; with the namespace where you want to create the web terminal.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are an admin, you must create your web terminal in &lt;code&gt;openshift-terminal&lt;/code&gt;, otherwise you will not be able to connect to the web terminal through the OpenShift web console. If the &lt;code&gt;openshift-terminal&lt;/code&gt; namespace does not currently exist, you must start your first web terminal through the UI so that the namespace is provisioned.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kind: DevWorkspace apiVersion: workspace.devfile.io/v1alpha2 metadata: name: web-terminal namespace: ${your namespace} annotations: controller.devfile.io/restricted-access: "true" labels: console.openshift.io/terminal: "true" spec: started: true routingClass: 'web-terminal' template: components: - name: web-terminal-exec plugin: kubernetes: name: web-terminal-exec namespace: openshift-operators - name: web-terminal-tooling plugin: kubernetes: name: web-terminal-tooling namespace: openshift-operators components: - name: web-terminal-tooling container: volumeMounts: - name: home-storage path: "/home/user" - name: home-storage volume: {}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once your web terminal custom resource has been created, you can start a web terminal and all your changes under &lt;code&gt;/home/user&lt;/code&gt; will be persisted by default.&lt;/p&gt; &lt;h2&gt;Tooling update&lt;/h2&gt; &lt;p&gt;We have updated the default binaries in Web Terminal Operator 1.3 to include the latest versions of the built-in command-line tools, as shown in Table 1.&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="388"&gt;&lt;caption&gt;Table 1: Command-line tools in Web Terminal Operator 1.3.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Binary&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;Old version&lt;/strong&gt;&lt;/th&gt; &lt;th&gt;&lt;strong&gt;New version&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;oc&lt;/code&gt;&lt;/td&gt; &lt;td&gt;4.7.0&lt;/td&gt; &lt;td&gt;4.8.2&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/td&gt; &lt;td&gt;v1.20.1&lt;/td&gt; &lt;td&gt;v0.21.0-beta.1&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;odo&lt;/code&gt;&lt;/td&gt; &lt;td&gt;2.0.4&lt;/td&gt; &lt;td&gt;2.2.3&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;knative&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.19.1&lt;/td&gt; &lt;td&gt;0.21.0&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;tekton&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.15.0&lt;/td&gt; &lt;td&gt;0.17.2&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;kubectx&lt;/code&gt;&lt;/td&gt; &lt;td&gt;v0.9.3&lt;/td&gt; &lt;td&gt;v0.9.4&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;kubens&lt;/code&gt;&lt;/td&gt; &lt;td&gt;v0.9.3&lt;/td&gt; &lt;td&gt;v0.9.4&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;rhoas&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.24.1&lt;/td&gt; &lt;td&gt;0.25.0&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;submariner&lt;/code&gt;&lt;/td&gt; &lt;td&gt; &lt;/td&gt; &lt;td&gt;0.9.1 (First release)&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;h2&gt;Try out these features&lt;/h2&gt; &lt;p&gt;In Web Terminal 1.3 we have changed the default channel from alpha to fast. This means that you’ll have to go through some extra steps to try out these new features:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;If your cluster already has the Web Terminal Operator installed, uninstall it by following the &lt;a href="https://docs.openshift.com/container-platform/4.7/web_console/odc-about-web-terminal.html#deleting-the-web-terminal-components-and-custom-resources"&gt;uninstall instructions&lt;/a&gt; and re-install the Web Terminal Operator using the fast channel.&lt;/li&gt; &lt;li&gt;If your cluster does not have the Web Terminal Operator installed, install it using the fast channel.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Additional resources&lt;/h2&gt; &lt;p&gt;For a peek into how the Web Terminal Operator works under the hood, please see &lt;a href="https://www.openshift.com/blog/a-deeper-look-at-the-web-terminal-operator-1"&gt;A deeper look at the Web Terminal Operator&lt;/a&gt; by Angel Misevski. You can also check out the initial release article by Joshua Wood: &lt;a href="https://developers.redhat.com/blog/2020/10/01/command-line-cluster-management-with-red-hat-openshifts-new-web-terminal-tech-preview/"&gt;Command-line cluster management with Red Hat OpenShift’s new web terminal&lt;/a&gt;. For a look at our previous release blog, read &lt;a href="https://developers.redhat.com/blog/2021/03/08/whats-new-in-red-hat-openshifts-web-terminal-operator-1-2"&gt;What’s new in Red Hat OpenShift's Web Terminal Operator 1.2&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/19/cluster-tooling-updates-and-more-red-hat-openshifts-web-terminal-operator-13" title="Cluster tooling updates and more in Red Hat OpenShift's Web Terminal Operator 1.3"&gt;Cluster tooling updates and more in Red Hat OpenShift's Web Terminal Operator 1.3&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/J3giX7F6Bw4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Josh Pinkney, Angel Misevski</dc:creator><dc:date>2021-08-19T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/19/cluster-tooling-updates-and-more-red-hat-openshifts-web-terminal-operator-13</feedburner:origLink></entry><entry><title type="html">Quarkus 2.1.3.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ygixatckB04/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-1-3-final-released/</id><updated>2021-08-19T00:00:00Z</updated><content type="html">We just released Quarkus 2.1.3.Final, our third maintenance release on top of 2.1. It is a safe upgrade for anyone already using 2.1. If you are not using 2.1 already, please refer to the 2.1 migration guide. Full changelog You can get the full changelog of 2.1.3.Final on GitHub. Come...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ygixatckB04" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-1-3-final-released/</feedburner:origLink></entry><entry><title>Connect to an external PostgreSQL database with SSL and Red Hat's single sign-on technology</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/WDsW_vxS2Eg/connect-external-postgresql-database-ssl-and-red-hats-single-sign-technology" /><author><name>Olivier Rivat</name></author><id>555ef949-fa8a-4a66-a0c8-69be98359164</id><updated>2021-08-18T07:00:00Z</updated><published>2021-08-18T07:00:00Z</published><summary type="html">&lt;p&gt;This article shows you how to connect securely to applications and data sources using &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on technology&lt;/a&gt;. The example connects to an external &lt;a href="https://www.postgresql.org"&gt;PostgreSQL&lt;/a&gt; database in secure Single Sockets Layer (SSL) mode, first locally and then on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. As you will see, it is usually much easier to carry out the integration first on a standalone instance of Red Hat's SSO, and then deploy it on OpenShift. At a high level, we will do the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Step 1: Configure the PostgreSQL server in SSL mode.&lt;/li&gt; &lt;li&gt;Step 2: Configure SSO to connect to the PostgreSQL server in SSL mode.&lt;/li&gt; &lt;li&gt;Step 3: Deploy SSO on OpenShift and connect to the PostgreSQL database using SSL.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Step 1: Configure the PostgreSQL server in SSL mode&lt;/h2&gt; &lt;p&gt;In this section, we install PostgreSQL and change some of its configuration files to enable SSL connections. We also create &lt;a href="https://developers.redhat.com/blog/2021/02/19/x-509-user-certificate-authentication-with-red-hats-single-sign-on-technology"&gt;X.509 certificates&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Install PostgreSQL on Red Hat Enterprise Linux 8&lt;/h3&gt; &lt;p&gt;This section describes how to how to install a PostgreSQL server on &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 8. You can find more details on Red Hat's &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/deploying_different_types_of_servers/using-databases#using-postgresql"&gt;Using PostgreSQL page&lt;/a&gt;. These commands have to be entered as superuser (root).&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# yum module install postgresql:13/server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;postgres&lt;/code&gt; superuser is created automatically. Next, initialize the database cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# postgresql-setup --initdb&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Red Hat recommends storing database data in the default &lt;code&gt;/var/lib/pgsql/data&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;Next, start the &lt;code&gt;postgresql&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl start postgresql.service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enable the &lt;code&gt;postgresql&lt;/code&gt; service to start at boot:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl enable postgresql.service&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure the postgres user password&lt;/h3&gt; &lt;p&gt;Choose a password for the &lt;code&gt;postgres&lt;/code&gt; user as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[root@node-0 data]# su - postgres Last login: Mon Aug 9 05:21:55 EDT 2021 on pts/0 [postgres@node-0 ~]$ psql psql (13.3) Type "help" for help. postgres=# \password postgres Enter new password: Enter it again: postgres=# &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure authentication&lt;/h3&gt; &lt;p&gt;The &lt;code&gt; /var/lib/pgsql/data/pg_hba.conf&lt;/code&gt; file configures client authentication for PostgreSQL databases. Upgrade this file as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# TYPE DATABASE USER ADDRESS METHOD local all all md5 host all all 0.0.0.0/0 md5 hostssl all all 0.0.0.0/0 scram-sha-256 &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure the connection&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;/var/lib/pgsql/data/postgresql.conf&lt;/code&gt; file sets database cluster parameters. Upgrade this file to include the following lines:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;listen_addresses = '*' # what IP address(es) to listen on; ... ... # - SSL - ssl = on ssl_ca_file = '/var/lib/pgsql/data/root.crt' ssl_cert_file = '/var/lib/pgsql/data/server.crt #ssl_crl_file = '' ssl_key_file = '/var/lib/pgsql/data/server.key'&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create X.509 certificates&lt;/h3&gt; &lt;p&gt;Log in as the &lt;code&gt;postgres&lt;/code&gt; user and create certificates in the &lt;code&gt;/var/lib/pgsql/data&lt;/code&gt; directory as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# cd /var/lib/pgsql/data # log as poostgres user # su - postgres $ openssl genrsa -out server.key 2048 $ openssl rsa -in server.key -out server.key $ chmod 400 server.key $ openssl req -new -key server.key -days 3650 -out server.crt -x509 -subj "/CN=example.com" $ openssl req -new -key server.key -days 3650 -out server.crt -x509 -subj "/CN=example.com" $ cp server.crt root.crt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then restart the PostgreSQL server (as root):&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl restart postgresql.service&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Test the connection to the remote PostgreSQL database&lt;/h3&gt; &lt;p&gt;From a remote machine, test that you can connect to your PostgreSQL database:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ psql -h node-0.postgresql.lab.pnq2.cee.redhat.com -U postgres -W Password for user postgres: psql (10.17, server 13.3) WARNING: psql major version 10, server major version 13. Some psql features might not work. Type "help" for help. postgres=#&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The prompt at the end of the example shows that you are logged in to a running instance of PostgreSQL.&lt;/p&gt; &lt;h2&gt;Step 2: Connect Red Hat's SSO to the PostgreSQL server using SSL&lt;/h2&gt; &lt;p&gt;Now that PostgreSQL is running and able to communicate over SSL, you can set up Red Hat's single sign-on technology to connect to the PostgreSQL database using a JBoss script.&lt;/p&gt; &lt;h3&gt;Create a database for Keycloak&lt;/h3&gt; &lt;p&gt;Connect to the PostgreSQL server and create a database for &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt;, the authentication service that Red Hat's SSO is based on. Start with a &lt;code&gt;CREATE DATABASE&lt;/code&gt; command in a format such as the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-sql"&gt;postgres-# CREATE DATABASE &lt;keycloak-database-name&gt;;&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If there is already an existing Keycloak database, you must delete it with a &lt;code class="language-sql"&gt;DROP DATABASE&lt;/code&gt; command before you create the one used for this example.&lt;/p&gt; &lt;p&gt;For our example, we'll create a database with the name &lt;code&gt;keycloak&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-sql"&gt;postgres-# CREATE DATABASE keycloak;&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The name of the database created will appear in the data source's connection URL.&lt;/p&gt; &lt;h3&gt;Download the PostgreSQL driver&lt;/h3&gt; &lt;p&gt;Download the &lt;code&gt;postgresql&lt;/code&gt; driver from the &lt;a href="https://jdbc.postgresql.org/download.html"&gt;PostgreSQL JDBC driver page&lt;/a&gt;. The driver is in a file named &lt;code&gt;postgresql-42.2.23.jar&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Use a JBoss script to connect to the external database&lt;/h3&gt; &lt;p&gt;A JBoss script named &lt;code&gt;sso-extensions.cli&lt;/code&gt; follows. It contains commands to make Keycloak use PostgreSQL instead of the H2 database that Keycloak uses by default. You need to run the script to allow Keycloak to connect to PostgreSQL in SSL mode.&lt;/p&gt; &lt;p&gt;Before you run the following script, replace the string &lt;code&gt;&lt;postgresql-server-hostname&gt;&lt;/code&gt; with the fully qualified domain name of the PostgreSQL server.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;batch set DB_USERNAME=postgres set DB_PASSWORD=postgres set DRIVER_NAME=postgres set DRIVER_MODULE_NAME=org.postgres set XA_DATABASESOURCE_CLASS="org.postgresql.xa.PGXADataSource" set CONNECTION_URL="jdbc:postgresql://&lt;postgresql-server-hostname&gt;:5432/keycloak?ssl=true;sslfactory=org.postgresql.ssl.NonValidatingFactory" set FILE=/tmp/postgresql-42.2.23.jar module add --name=$DRIVER_MODULE_NAME --resources=$FILE --dependencies=javax.api,javax.resource.api /subsystem=datasources/jdbc-driver=$DRIVER_NAME:add( \ driver-name=$DRIVER_NAME, \ driver-module-name=$DRIVER_MODULE_NAME, \ xa-datasource-class=$XA_DATABASESOURCE_CLASS \ ) /subsystem=datasources/data-source=KeycloakDS:remove() /subsystem=datasources/data-source=KeycloakDS:add( \ jndi-name=java:jboss/datasources/KeycloakDS, \ enabled=true, \ use-java-context=true, \ connection-url=$CONNECTION_URL, \ driver-name=$DRIVER_NAME, \ user-name=$DB_USERNAME, \ password=$DB_PASSWORD \ ) run-batch &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Test the PostgreSQL database connection&lt;/h3&gt; &lt;p&gt;Now, check whether the previous steps let you connect safely to the PostgreSQL database in SSL mode:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ unzip rh-sso-7.4.0.zip $ cd &lt;rhsso-install-dir&gt; $ bin/sh standalone.sh # Run the PostgreSQL CLI script $ bin/jboss-cli.sh --connect --file=sso-extensions.cli&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, visit the URL &lt;code&gt;https://localhost:8443/auth&lt;/code&gt; to see that the connection is successful.&lt;/p&gt; &lt;h2&gt;Step 3: Deploy SSO on OpenShift and connect to the PostgreSQL database using SSL&lt;/h2&gt; &lt;p&gt;Now we'll move to OpenShift. This section deploys Red Hat's SSO on an OpenShift cluster and connects from there to the external PostgreSQL database in SSL mode. The steps are:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Build a new SSO Docker image using the JBoss command file shown previously.&lt;/li&gt; &lt;li&gt;Deploy Red Hat's SSO on OpenShift using the standard &lt;code&gt;sso74-x509-https&lt;/code&gt; template.&lt;/li&gt; &lt;li&gt;Update the SSO deployment configuration to use the new SSO image.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Build an SSO Docker image&lt;/h3&gt; &lt;p&gt;We'll build a new SSO Docker image to allow connections to the external PostgreSQL driver using SSL. This process is described in detail in the Red Hat documentation for &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.4/html/red_hat_single_sign-on_for_openshift_on_openjdk/advanced_concepts#sso-using-custom-jdbc-driver"&gt;using a custom JDBC driver&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Create a new directory and install the following files there:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;postgresql-42.2.23.jar&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;sso-extensions.cli&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Update the &lt;code&gt;sso-extensions.cli&lt;/code&gt; JBoss script to install the driver from the location in &lt;code&gt;/opt/eap/extensions/jdbc-driver.jar:&lt;/code&gt;&lt;/p&gt; &lt;pre&gt; &lt;code&gt;batch set DB_USERNAME=postgres set DB_PASSWORD=postgres set DRIVER_NAME=postgres set DRIVER_MODULE_NAME=org.postgres set XA_DATABASESOURCE_CLASS="org.postgresql.xa.PGXADataSource" set CONNECTION_URL="jdbc:postgresql://&lt;postgresl-server-hostname&gt;:5432/keycloak?ssl=true;sslfactory=org.postgresql.ssl.NonValidatingFactory" set FILE=/opt/eap/extensions/jdbc-driver.jar module add --name=$DRIVER_MODULE_NAME --resources=$FILE --dependencies=javax.api,javax.resource.api /subsystem=datasources/jdbc-driver=$DRIVER_NAME:add( \ driver-name=$DRIVER_NAME, \ driver-module-name=$DRIVER_MODULE_NAME, \ xa-datasource-class=$XA_DATABASESOURCE_CLASS \ ) /subsystem=datasources/data-source=KeycloakDS:remove() /subsystem=datasources/data-source=KeycloakDS:add( \ jndi-name=java:jboss/datasources/KeycloakDS, \ enabled=true, \ use-java-context=true, \ connection-url=$CONNECTION_URL, \ driver-name=$DRIVER_NAME, \ user-name=$DB_USERNAME, \ password=$DB_PASSWORD \ ) run-batch &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Dockerfile contains:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;FROM rh-sso-7/sso74-openshift-rhel8:latest COPY sso-extensions.cli /opt/eap/extensions/ COPY postgresql-42.2.23.jar /opt/eap/extensions/jdbc-driver.jar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, build a new SSO image using Podman:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ podman build -t localhost/docker-registry-default/project/sso74-external-db-postgres-ssl:1.0 . STEP 1: FROM rh-sso-7/sso74-openshift-rhel8:latest STEP 2: COPY sso-extensions.cli /opt/eap/extensions/ --&gt; 9f79713bfc3 STEP 3: COPY postgresql-42.2.23.jar /opt/eap/extensions/jdbc-driver.jar STEP 4: COMMIT localhost/docker-registry-default/project/sso74-external-db-postgres-ssl:1.0 --&gt; af34362aeab af34362aeabbdaeb4c3319e42ff8f20c7e3a9cbf6031b6f60301a7ba83d4e558&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Push the new SSO image to quay.io:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;podman login quay.io -u &lt;username&gt; -p &lt;password&gt; $ podman push localhost/docker-registry-default/project/sso74-external-db-postgres-ssl:1.0 quay.io/&lt;username&gt;/sso74-external-db-postgres-ssl Getting image source signatures Copying blob fa592e808c80 done Copying blob 329b07dcfb80 done Copying blob 69fa687f24b7 skipped: already exists Copying blob 870b2c4dba9d skipped: already exists Copying blob 1e3f73167579 skipped: already exists Copying config af34362aea done Writing manifest to image destination Copying config af34362aea [--------------------------------------] 0.0b / 4.4KiB Writing manifest to image destination Storing signatures &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Configure Red Hat's SSO on OpenShift&lt;/h3&gt; &lt;p&gt;Create a new project in OpenShift and deploy the &lt;code&gt;sso74-x509-https&lt;/code&gt; template there. This template initially connects to the default H2 database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project sso-74-external-db-ssl Now using project "sso-74-external-db-ssl" on server "https://openshift.example.com:443" $ oc process sso74-x509-https SSO_ADMIN_USERNAME=admin SSO_ADMIN_PASSWORD=password -n openshift -o yaml &gt; sso74-x509-https.yaml $ oc create -f sso74-x509-https.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can now check the status of the SSO server:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ oc get pods NAME READY STATUS RESTARTS AGE sso-1-x625p 1/1 Running 0 42s $ oc status In project sso-74-external-db-ssl on server https://openshift.example.com:443 svc/sso-ping (headless):8888 https://sso-sso-74-external-db-ssl.apps.example.com (reencrypt) (svc/sso) dc/sso deploys openshift/sso74-openshift-rhel8:7.4 deployment #1 deployed about a minute ago - 1 pod View details with 'oc describe &lt;resource&gt;/&lt;name&gt;' or list everything with 'oc get all'. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also connect to the SSO admin console at the URL &lt;code&gt;https://sso-sso-74-external-db-ssl.apps.example.com/auth&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Update the deployment with the new SSO image&lt;/h3&gt; &lt;p&gt;Update the deployment configuration as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc edit dc/sso&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace the SSO image with:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; ... ... image: quay.io/orivat/sso74-external-db-postgres-ssl:latest imagePullPolicy: Always livenessProbe: .... .... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also update the &lt;code&gt;triggers&lt;/code&gt; section so that it pulls new SSO images automatically from quay.io instead of from Red Hat's SSO registry:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;.... .... triggers: - imageChangeParams: automatic: true containerNames: - sso from: kind: ImageStreamTag name: sso74-openshift-rhel8:7.4 namespace: openshift type: ImageChange - type: ConfigChange .... ....&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Test the SSO server status&lt;/h3&gt; &lt;p&gt;You can see the status of the SSO server as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; oc status In project sso-74-external-db-ssl on server https://openshift.example.com:443 svc/sso-ping (headless):8888 https://sso-sso-74-external-db-ssl.apps.example.com (reencrypt) (svc/sso) dc/sso deploys quay.io/&lt;username&gt;/sso74-external-db-postgres-ssl:latest deployment #2 failed 34 minutes ago: config change deployment #1 deployed about an hour ago - 1 pod &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output indicates that it has pulled the image from quay.io.&lt;/p&gt; &lt;p&gt;It is now possible to connect safely from the SSO admin console to the external PostgreSQL database in SSL mode at the following URL:&lt;/p&gt; &lt;pre&gt; https://sso-sso-74-external-db-ssl.apps.example.com/auth&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article showed you how to use Red Hat's single sign-on technology to connect from OpenShift to an external PostgreSQL database over SSL. We used a new custom SSO Docker image, which contains the PostgreSQL driver and a JBoss configuration script to connect to the external database in SSL mode. You can generalize the approach we've followed here to any Openshift project using Red Hat's SSO.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/18/connect-external-postgresql-database-ssl-and-red-hats-single-sign-technology" title="Connect to an external PostgreSQL database with SSL and Red Hat's single sign-on technology"&gt;Connect to an external PostgreSQL database with SSL and Red Hat's single sign-on technology&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/WDsW_vxS2Eg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Olivier Rivat</dc:creator><dc:date>2021-08-18T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/18/connect-external-postgresql-database-ssl-and-red-hats-single-sign-technology</feedburner:origLink></entry><entry><title type="html">LRA annotation checker Maven plugin</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/4YkGENDajkI/lra-annotation-checker-maven-plugin.html" /><author><name>Ondra Chaloupka</name></author><id>https://jbossts.blogspot.com/2021/08/lra-annotation-checker-maven-plugin.html</id><updated>2021-08-17T10:56:00Z</updated><content type="html">With the release of the LRA (Long Running Actions) specification in Narayana team works on integrating the to various application runtimes. Currently it's and . (Camel is a third platform but it does in a way not depending on LRA annotations defined in the specification.). NOTE:If you want to get introduction what is LRA and what is good for you can read some of the already published articles (, ). At this time when the LRA  can be easily grab and used within the application runtimes it may come some difficulty on precise use of the . The defines different "requirements" the LRA application has to follow to work correctly. Some are basic as "the LRA annotated class must contain at least one of the methods annotated with @Compensate or @AfterLRA" or that the LRA annotated JAX-RS endpoints has predefined HTTP methods to be declared with. For example the /requires the @PUT method while the requires the @DELETE and requires the @GET. When the specific LRA contract rule is violated the developer will find them at the deployment time with the being thrown. But time of the deployment could be a bit late to find just a forgotten annotation required by the LRA specification. With that idea in mind Narayana offers a Maven plugin project . The developer working with the LRA introduces the dependency to the LRA annotation with artifact . He codes the application and then he can introduce the Maven plugin of the LRA checker to be run during Maven phase process-classes. The developer needs to point to the plugin goal check to get the verification being run. The snippets that can be placed to the project pom.xml is following. The plugin Maven artifact coordinates is io.narayana:maven-plugin-lra-annotations_1.0:1.0.0.Beta1 ... &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;io.narayana&lt;/groupId&gt; &lt;artifactId&gt;maven-plugin-lra-annotations_1.0&lt;/artifactId&gt; &lt;version&gt;1.0.0.Beta1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;check&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ... When plugin is loaded it searches for classes available at path ${project.build.directory}/classes(i.e., target/classes) and tries to find if the application preserve the rules defined by the LRA specification. When not then the Maven build fails reporting what error happens. Such an error is in format of [error id]:[description]. Example of is [ERROR] Failed to execute goal io.narayana:maven-plugin-lra-annotations_1.0:1.0.0.Beta1:check (default) on project lra-annotation-checker-maven-plugin-test: LRA annotation errors: [ERROR] [[4]]-&gt;   1: The class annotated with org.eclipse.microprofile.lra.annotation.ws.rs.LRA missing at least one of the annotations Compensate or AfterLRA Class: io.narayana.LRAParticipantResource;   2: Multiple annotations of the same type are used. Only one per the class is expected. Multiple annotations 'org.eclipse.microprofile.lra.annotation.Status' in the class 'class io.narayana.LRAParticipantResource' on methods [status, status2].;   4: Wrong method signature for non JAX-RS resource method. Signature for annotation 'org.eclipse.microprofile.lra.annotation.Forget' in the class 'io.narayana.LRAParticipantResource' on method 'forget'. It should be 'public void/CompletionStage/ParticipantStatus forget(java.net.URI lraId, java.net.URI parentId)';   5: Wrong complementary annotation of JAX-RS resource method. Method 'complete' of class 'class io.narayana.LRAParticipantResource' annotated with 'org.eclipse.microprofile.lra.annotation.Complete' misses complementary annotation javax.ws.rs.PUT. The plugin can be configured with two parameters (placed under &lt;configuration&gt;under the &lt;plugin&gt; element). Attribute Description Default paths Paths searched for classes to be checked. Point to a directory or jar file. Multiple paths are delimited with a comma. ${project.build.directory}/classes failWhenPathNotExist When some path defined within argument paths does not exist then Maven build may fail or resume with information that the path is not available. true All the points described in this article can be seen and tested in an example at . The plugin configuration can be seen in the of the same project. Any comments, ideas for enhancement and bug reports are welcomed. The project LRA annotation checker Maven plugin is placed in the . The issues can be submitted via JBTM issue tracker at . Hopefully this small plugin provides a better experience for any developer working with the LRA and LRA Narayana implementation in particular.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/4YkGENDajkI" height="1" width="1" alt=""/&gt;</content><dc:creator>Ondra Chaloupka</dc:creator><feedburner:origLink>https://jbossts.blogspot.com/2021/08/lra-annotation-checker-maven-plugin.html</feedburner:origLink></entry><entry><title>Explore new features in SystemTap 4.5.0</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3l_i9rAxnAU/explore-new-features-systemtap-450" /><author><name>Stan Cox</name></author><id>de3aa528-f93b-4663-b244-41271b5a78bb</id><updated>2021-08-16T07:00:00Z</updated><published>2021-08-16T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/systemtap_beginners_guide/understanding-how-systemtap-works"&gt;SystemTap&lt;/a&gt; uses a command-line interface (CLI) and a scripting language to write instrumentation for a live, running kernel or a user-space application. A SystemTap script associates handlers with named events. When a specified event occurs, the default SystemTap kernel runtime runs the handler in the kernel like a quick subroutine and then resumes.&lt;/p&gt; &lt;p&gt;This article lays out the new features in SystemTap 4.5.0. This version will appear in &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; 9.0. The features fall into three general categories: context variable access, aliasing, and the &lt;a href="https://www.kernel.org/doc/html/latest/bpf/index.html"&gt;Berkeley Packet Filter (BPF)&lt;/a&gt; back-end.&lt;/p&gt; &lt;h2&gt;Context variable improvements&lt;/h2&gt; &lt;p&gt;These features deal with enumerator access, thread-local storage access, floating-point variable access, and context variable access within functions.&lt;/p&gt; &lt;h3&gt;Enumerator access&lt;/h3&gt; &lt;p&gt;Enumerator values can now be accessed as &lt;code&gt;$context&lt;/code&gt; variables, specified through a dollar sign. For example, consider the following C program source code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;typedef enum { at='@',sharp='#' } symbols; symbols symbol = at;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Suppose you run the following probe handler statement:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;printf("symbol=%c\n",$symbol)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The statement displays:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;symbol=@&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Only unscoped enumerators are currently supported; scoped enumerators such as the following are not:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;enum class Color { red, green = 20, blue };&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Implicit thread-local storage&lt;/h3&gt; &lt;p&gt;Implicit thread-local storage consists of variables that have a different instance in each thread and persist as long as the thread is alive. The following C declaration defines an implicit thread-local variable named &lt;code&gt;tls&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt; __thread unsigned long tls = 99;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each thread has its own instance of &lt;code&gt;tls&lt;/code&gt; and can change it without affecting the variable of the same name in other threads. On GNU/Linux systems, you can obtain more information about thread-local variables through the command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;info gcc 'C Extensions' Thread-Local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If a probe handler is defined for the same module that defines &lt;code&gt;tls&lt;/code&gt;, the handler can access the value via &lt;code&gt;@var("tls")&lt;/code&gt;. If &lt;code&gt;tls&lt;/code&gt; is defined in another module, e.g., the shared object &lt;code&gt;libtls.so&lt;/code&gt;, the value of &lt;code&gt;tls&lt;/code&gt; can be accessed via &lt;code&gt;@var("tls","libtls.so")&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;One common use of this feature is to read the &lt;code&gt;errno&lt;/code&gt; value, which is set by many library functions. Take for example the following C code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;infile = fopen (file_does_not_exist, "r");&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When a statement like this one tries to open a file that doesn't exist, the system returns an &lt;code&gt;errno&lt;/code&gt; value of &lt;code&gt;ENOENT&lt;/code&gt;. After issuing the &lt;code&gt;fopen&lt;/code&gt; call, assume you run the following probe handler statement:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;printf("errno=%d %s\n",@errno,errno_str(@errno))}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The statement displays:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;errno=2 ENOENT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Support for tracking virtual memory addresses was added to the stapdyn back-end. That feature enables the SystemTap Dyninst back-end to access global and static variables, including thread-local variables.&lt;/p&gt; &lt;p&gt;Implicit thread-local variables can be accessed as &lt;code&gt;$context&lt;/code&gt; variables on the x86_64, PowerPC, and s390 architectures.&lt;/p&gt; &lt;h3&gt;Floating-point variables&lt;/h3&gt; &lt;p&gt;A SystemTap probe handler can now access floating-point variables as &lt;code&gt;$context&lt;/code&gt; variables. Start with the following C declaration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;float pi = 3.14159&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the probe handler statement:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;printf("%s\n",fp_to_string($pi,5))&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The statement displays:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;3.14159&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;%e&lt;/code&gt; and &lt;code&gt;%f&lt;/code&gt; conversion specifiers of &lt;code&gt;printf&lt;/code&gt; are not currently supported by the SystemTap &lt;code&gt;printf&lt;/code&gt; statement. The &lt;code&gt;fp_to_string&lt;/code&gt; function can be used instead. Other useful floating-point functions include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;string_to_fp (string)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;long_to_fp (long)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_to_long (float)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_add (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_sub (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_mul (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_div (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_sqrt (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_eq (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_le (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;fp_lt (float1, float2)&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;32-bit floats are automatically widened to doubles.&lt;/p&gt; &lt;h3&gt;Access context variables inside functions&lt;/h3&gt; &lt;p&gt;Functions may now refer to &lt;code&gt;$context&lt;/code&gt; variables and to operators that reference &lt;code&gt;$context&lt;/code&gt; variables such as &lt;code&gt;$$vars&lt;/code&gt; and &lt;code&gt;$$locals&lt;/code&gt;. Previously these references could appear only inside probe handlers.&lt;/p&gt; &lt;p&gt;Consider the following C code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;int handle_object1 () { int attr1; int attr2; attr1 = 9; attr2 = 99; types type = thing1; return 0; } int handle_object2 () { int attr1; int attr2; attr1 = 8; attr2 = 88; types type = thing2; return 0; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Probe handlers can refer to a common &lt;code&gt;binary_op_vals&lt;/code&gt; function that accesses the context variables &lt;code&gt;attr1&lt;/code&gt; and &lt;code&gt;attr2&lt;/code&gt;, which are common to both the &lt;code&gt;handle_object1&lt;/code&gt; and &lt;code&gt;handle_object2&lt;/code&gt; functions. The &lt;code&gt;binary_op_vals&lt;/code&gt; function can also refer to the &lt;code&gt;$context&lt;/code&gt; variable operators $$&lt;code&gt;vars&lt;/code&gt; and $$&lt;code&gt;locals&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;function binary_op_vals () { printf ("type = %d attribute 1 = %d attribute 2 = %d\n", $type, $attr1, $attr2); } probe process.statement("handle_object1@*:11") { binary_op_vals (); } probe process.statement("handle_object2@*:22") { binary_op_vals (); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The preceding probe handlers display:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;type = 0 attribute 1 = 9 attribute 2 = 99 type = 1 attribute 1 = 8 attribute 2 = 88&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;BPF back-end improvements&lt;/h2&gt; &lt;p&gt;Values in user space can now be accessed with functions such as &lt;code&gt;user_string&lt;/code&gt;, &lt;code&gt;user_int&lt;/code&gt;, and &lt;code&gt;user_long&lt;/code&gt;. Consider the following C source code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;long ipf = 12; long *ipfp = &amp;ipf;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then run the following &lt;code&gt;stap&lt;/code&gt; command from the SystemTap CLI:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;stap --bpf --disable-cache -e 'probe process.statement("main@*:22") {printf("%d\n",user_long($ipfp))}' -c ./tstustr&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command displays:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;12&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note the use of the &lt;code&gt;-c&lt;/code&gt; option to specify the target program. The &lt;code&gt;systemtap -c &lt;command&gt;&lt;/code&gt; option can now be used with the BPF back-end. This option sets the SystemTap target process to the process ID (PID) of the running command.&lt;/p&gt; &lt;h2&gt;Aliasing improvements&lt;/h2&gt; &lt;p&gt;An alias provides a mechanism for augmenting the handlers that are taken by another probe. The alias handlers can be invoked either before the handler, known as a &lt;em&gt;prologue&lt;/em&gt;, or after the handler, known as an &lt;em&gt;epilogue&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;It is now possible to define an alias with both a prologue and an epilogue. For example, a probe follows that has a prologue and an epilogue. The probe sets the global variable &lt;code&gt;file_descriptors&lt;/code&gt; whenever the &lt;code&gt;open&lt;/code&gt; or &lt;code&gt;openat&lt;/code&gt; syscalls are invoked:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt;private global filenames private global file_descriptors probe file_open = syscall.{open,openat} { delete filenames[tid()] },{ if (! (tid() in filenames)) filenames[tid()] = filename; } probe file_open_return = syscall.{open.return,openat.return} { if (tid() in filenames) file_descriptors[tid(),retval] = filenames[tid()] } // These alias definitions can be used to only record file descriptors // that match a given pathname. probe file_open { %($# &gt; 0 %? if (strpos(filename,@1) == -1) next %) } probe file_open_return { } probe end { foreach ([t,f] in file_descriptors) { printf ("%d %d %s\n", t, f, file_descriptors[t,f]) } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If this script is named &lt;code&gt;alias.stp&lt;/code&gt;, you can execute it through:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;stap alias.stp /home/user&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The script displays output such as:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;2309651 101 "/home/user/path1" 2310349 459 "/home/user/path2"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Additionally, the &lt;code&gt;@probewrite&lt;/code&gt; predicate can be used to determine whether a variable has already been written to. For example, the prologue just shown could be defined as follows, to check that &lt;code&gt;filesnames&lt;/code&gt; has already been written to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cs"&gt; if (@probewrite(filenames)) delete filenames[tid()]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;SystemTap in Red Hat Enterprise Linux 9 fills several gaps left in previous versions and provides many conveniences to developers investigating kernel and application behavior. These new features might inspire you to try moving to a lower level of the system through SystemTap.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/16/explore-new-features-systemtap-450" title="Explore new features in SystemTap 4.5.0"&gt;Explore new features in SystemTap 4.5.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3l_i9rAxnAU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Stan Cox</dc:creator><dc:date>2021-08-16T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/16/explore-new-features-systemtap-450</feedburner:origLink></entry><entry><title>Test container images in Red Hat OpenShift 4 with Ansible and CI/CD</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/bljgoDky4Wg/test-container-images-red-hat-openshift-4-ansible-and-cicd" /><author><name>Petr Hracek</name></author><id>006c2cb0-68fe-4e13-8f71-d003e718625e</id><updated>2021-08-13T07:00:00Z</updated><published>2021-08-13T07:00:00Z</published><summary type="html">&lt;p&gt;Several repositories offer ready-made &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images for &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; and other systems running &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;. The InterOp team at Red Hat tests these application images in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. To simplify the integration of tests into the &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration/continuous delivery (CI/CD)&lt;/a&gt; process, we are adding &lt;a href="https://www.ansible.com/"&gt;Ansible&lt;/a&gt; playbooks to the repositories that host the container images. The Red Hat Software Collections &lt;a href="https://github.com/sclorg/"&gt;GitHub repository&lt;/a&gt; currently has the first of these Ansible playbooks, but we will add playbooks to other repositories over time.&lt;/p&gt; &lt;p&gt;This article shows how to submit a test to a repository, and how to download the tests if you want to run them in your own container environment.&lt;/p&gt; &lt;h2&gt;Parameters for testing a container&lt;/h2&gt; &lt;p&gt;In order to test a container under a Red Hat OpenShift 4 environment, the developer has to provide information about where to download, deploy, and test the container. The necessary information is illustrated in the following playbook for a PostgreSQL container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;registry_redhat_io: "rhscl/postgresql-10-rhel7" tag_name: "postgresql:10-el7" deployment: "oc new-app postgresql:10-el7~https://github.com/sclorg/postgresql-container.git \ --name new-postgresql \ --context-dir examples/extending-image/ \ -e POSTGRESQL_USER=user \ -e POSTGRESQL_DATABASE=db \ -e POSTGRESQL_PASSWORD=password" pod_name: "new-postgresql" add_route: true test_exec_command: "./files/check_postgresql_container.sh" expected_exec_result: "FINE" check_curl_output: “SOMETHING from curl output” scl_url: "postgresql-container" is_name: "postgresql"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The meanings of the fields follow:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;registry_redhat_io&lt;/code&gt;: The image in the registry.redhat.io catalog, including the namespace, which is &lt;code&gt;rhscl&lt;/code&gt; in this case.&lt;/li&gt; &lt;li&gt;&lt;code&gt;tag_name&lt;/code&gt;: The tag name of the image.&lt;/li&gt; &lt;li&gt;&lt;code&gt;deployment&lt;/code&gt;: The command that deploys the image into the OpenShift environment.&lt;/li&gt; &lt;li&gt;&lt;code&gt;pod_name&lt;/code&gt;: name of the pod in the OpenShift namespace.&lt;/li&gt; &lt;li&gt;&lt;code&gt;add_route&lt;/code&gt;: Whether the route should be exposed, where the default is not to expose it.&lt;/li&gt; &lt;li&gt;&lt;code&gt;test_exec_command&lt;/code&gt;: The file that performs the test.&lt;/li&gt; &lt;li&gt;&lt;code&gt;expected_exec_result&lt;/code&gt;: A string expected from executing the &lt;code&gt;test_exec_command&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;check_curl_output&lt;/code&gt;: A substring from the expected output of a &lt;code&gt;curl&lt;/code&gt; command.&lt;/li&gt; &lt;li&gt;&lt;code&gt;scl_url&lt;/code&gt;: The repository name, for the Software Collections repositories only.&lt;/li&gt; &lt;li&gt;&lt;code&gt;is_name&lt;/code&gt;: The imagestream of the container.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To submit a test for public use, file a pull request at the site hosting the tests. There is currently one site at the &lt;a href="https://github.com/sclorg/ansible-tests/pulls"&gt;Software Collections GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Storing a test locally&lt;/h2&gt; &lt;p&gt;If you want to keep the test in your private environment instead of sharing the test, you can download our test suite and add your test to it as follows.&lt;/p&gt; &lt;p&gt;Clone the test repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/sclorg/ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go to the cloned repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add your container test suite to the &lt;a href="https://github.com/sclorg/ansible-tests/blob/master/deploy-and-test.yml"&gt;main Ansible playbook&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Running a test&lt;/h2&gt; &lt;p&gt;This section assumes that you are running an OpenShift 4 cluster.&lt;/p&gt; &lt;h3&gt;Downloading the OpenShift 4 client&lt;/h3&gt; &lt;p&gt;The latest version of the OpenShift 4 client, 4.6.18, can be obtained for your system at &lt;a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.618/"&gt;this mirror site&lt;/a&gt;. Download the ZIP file and unpack it through:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ tar -xzvf &lt;FILE&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The kubeconfig file&lt;/h3&gt; &lt;p&gt;Tests refer to the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/"&gt;kubeconfig&lt;/a&gt; file, so you need to point the &lt;code&gt;KUBECONFIG&lt;/code&gt; environment variable to the file. Ask your OpenShift 4 cluster administrator for the location of the file, then insert the path into the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export KUBECONFIG=&lt;path_to_kubeconfig&gt;/kubeconfig&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Running your test&lt;/h3&gt; &lt;p&gt;Switch to the cloned repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd ansible-tests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execute the test as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ make ocp4-tests EXT_TEST=&lt;your_test_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details are available in a &lt;a href="https://github.com/sclorg/ansible-tests/blob/master/README_ocp4.md"&gt;README file&lt;/a&gt; in the repository.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article showed a new method that makes it easy to add a test for your container to a test suite and run the test in an OpenShift 4 cluster that has been set up by an administrator. You can keep the test in your own environment, but we recommend that you heed the Red Hat phrase, "It’s better to share."&lt;/p&gt; &lt;p&gt;By providing your test to the InterOp team, you can get the container tested during each feature freeze or code freeze done by the OpenShift 4 development team. Feel free to &lt;a href="https://issues.redhat.com/projects/LPINTEROP/issues/LPINTEROP-1858?filter=allopenissues"&gt;contact the InterOp team at Red Hat&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/13/test-container-images-red-hat-openshift-4-ansible-and-cicd" title="Test container images in Red Hat OpenShift 4 with Ansible and CI/CD"&gt;Test container images in Red Hat OpenShift 4 with Ansible and CI/CD&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/bljgoDky4Wg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Petr Hracek</dc:creator><dc:date>2021-08-13T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/13/test-container-images-red-hat-openshift-4-ansible-and-cicd</feedburner:origLink></entry></feed>
